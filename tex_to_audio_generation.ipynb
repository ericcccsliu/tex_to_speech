{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b60cf6e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T21:16:04.305789Z",
     "start_time": "2024-03-02T21:16:04.291226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8086454",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T21:16:04.557067Z",
     "start_time": "2024-03-02T21:16:04.550984Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f26fc8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T21:16:04.821976Z",
     "start_time": "2024-03-02T21:16:04.818148Z"
    }
   },
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1475f2c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T21:16:05.247203Z",
     "start_time": "2024-03-02T21:16:05.218431Z"
    }
   },
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb77c447",
   "metadata": {},
   "source": [
    "## prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7dea92b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T21:26:05.163262Z",
     "start_time": "2024-03-02T21:26:05.159164Z"
    }
   },
   "outputs": [],
   "source": [
    "system_prompt = r\"\"\"I want to generate a podcast from LaTeX code. Please take the following LaTeX code and transcribe all content into a format optimized for text-to-speech. Do not make any effort to summarize or compress contentâ€“all original words by the author must be preserved. All text content should be preserved or transcribed into a easily readable format. Equations and math should be transcribed such that they are human readable in text. For example, $a^2$ should be transcribed as 'a squared'. Furthermore, all commands should also be transcribed to readable text. For example, commands such as \\section and \\title should be read as 'section' and 'title' respectively, and \\cite or \\citet should transcribe the citation as an in-text citation. Figures, tables, and comments must be omitted in their entirety.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ea7e46",
   "metadata": {},
   "source": [
    "## helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "203f0501",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T21:16:06.860218Z",
     "start_time": "2024-03-02T21:16:06.853555Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_latex_by_section(latex_content):\n",
    "    # Pattern to match section and subsection commands\n",
    "    pattern = r'(\\\\section\\{.*?\\}|\\\\subsection\\{.*?\\})'\n",
    "    \n",
    "    # Split the content by the pattern, keeping the delimiters\n",
    "    parts = re.split(pattern, latex_content)\n",
    "    \n",
    "    # Combine each command with its following content\n",
    "    combined_parts = []\n",
    "    for i in range(1, len(parts) - 1, 2):\n",
    "        combined_parts.append(parts[i] + parts[i + 1])\n",
    "    \n",
    "    # Add the last part if it doesn't end with a command\n",
    "    if len(parts) % 2 == 1:\n",
    "        combined_parts.append(parts[-1])\n",
    "    \n",
    "    return combined_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71a6aca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T21:16:07.373553Z",
     "start_time": "2024-03-02T21:16:07.366143Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_audio(snippet, index):\n",
    "    speech_file_path = f\"data/snippet_{index}.mp3\"\n",
    "    response = client.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"alloy\",\n",
    "        input=snippet\n",
    "    )\n",
    "    response.stream_to_file(speech_file_path)\n",
    "    return speech_file_path\n",
    "\n",
    "def generate_snippet(section):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": section}\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        max_tokens=4096,\n",
    "        n=1\n",
    "    )\n",
    "    snippet = chat_completion.choices[0].message.content\n",
    "    return snippet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35438cef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T21:14:39.402518Z",
     "start_time": "2024-03-02T21:14:39.397960Z"
    }
   },
   "source": [
    "## generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b29ef059",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T21:20:16.362568Z",
     "start_time": "2024-03-02T21:16:08.539484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paste raw LaTeX here: \\documentclass{article} % For LaTeX2e \\usepackage{iclr2023_conference,times}  % Optional math commands from https://github.com/goodfeli/dlbook_notation. \\input{math_commands.tex}  \\usepackage{hyperref} \\usepackage{url} \\usepackage{booktabs}       % professional-quality tables \\usepackage{amsfonts}       % blackboard math symbols \\usepackage{nicefrac}       % compact symbols for 1/2, etc. \\usepackage{microtype}      % microtypography \\usepackage{xcolor}         % colors \\usepackage{graphicx} \\usepackage{amssymb} \\usepackage{tabularx} \\usepackage[ruled,vlined]{algorithm2e} \\usepackage{amsmath} \\usepackage{subcaption} \\usepackage{multirow} \\usepackage{todonotes}  \\newcommand{\\TODO}[1]{\\textcolor{red}{(TODO: #1)}} \\newcommand{\\mw}[1]{\\textcolor{blue}{(MW: #1)}} \\newcommand{\\kl}[1]{\\textcolor[rgb]{0,0.545,0.545}{[KL: #1]}} \\newcommand{\\fv}[1]{\\textcolor[rgb]{0.85, 0, 0}{[Fernanda: #1]}} \\newcommand{\\db}[1]{\\textcolor[rgb]{0.8,0,0}{[@David: #1]}} \\newcommand{\\hp}[1]{\\textcolor[rgb]{1,0.1,0}{[HP: #1]}} \\newcommand{\\ah}[1]{\\textcolor[rgb]{.219,.4,0}{[Aspen: #1]}}  % delete all comments % \\newcommand{\\TODO}[1]{} % \\newcommand{\\mw}[1]{} % \\newcommand{\\kl}[1]{} % \\newcommand{\\fv}[1]{} % \\newcommand{\\db}[1]{} % \\newcommand{\\hp}[1]{} % \\newcommand{\\ah}[1]{}  \\title{Emergent world representations: Exploring a sequence model trained on a synthetic task}  % Authors must not appear in the submitted version. They should be hidden % as long as the \\iclrfinalcopy macro remains commented out below. % Non-anonymous submissions will be rejected without review.  \\author{% Kenneth Li\\thanks{Correspondence to \\texttt{ke\\_li@g.harvard.edu}} \\\\ Harvard University \\And Aspen K. Hopkins \\\\ Massachusetts Institute of Technology \\And David Bau \\\\ Northeastern University \\AND Fernanda Vi\\'egas \\\\ Harvard University \\And Hanspeter Pfister \\\\ Harvard University \\And Martin Wattenberg \\\\ Harvard University }  % The \\author macro works with any number of authors. There are two commands % used to separate the names and addresses of multiple authors: \\And and \\AND. % % Using \\And between authors leaves it to \\LaTeX{} to determine where to break % the lines. Using \\AND forces a linebreak at that point. So, if \\LaTeX{} % puts 3 of 4 authors names on the first line, and the last on the second % line, try using \\AND instead of \\And before the third author name.  % \\newcommand{\\fix}{\\marginpar{FIX}} % \\newcommand{\\new}{\\marginpar{NEW}}  \\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission. \\begin{document}  \\maketitle  \\begin{abstract}  Language models show a surprising range of capabilities, but the source of their apparent competence is unclear. Do these networks just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see? We investigate this question in a synthetic setting by applying a variant of the GPT model to the task of predicting legal moves in a simple board game, Othello. Although the network has no a priori knowledge of the game or its rules, we uncover evidence of an emergent nonlinear internal representation of the board state. Interventional experiments indicate this representation can be used to control the output of the network. By leveraging these intervention techniques, we produce ``latent saliency maps'' that help explain predictions. \\footnote{Codes at \\url{https://github.com/likenneth/othello\\_world}}  \\end{abstract}  \\section{Introduction}  Recent language models have shown an intriguing range of capabilities. Networks trained on a simple ``next-word'' prediction task are apparently capable of many other things, such as solving logic puzzles or writing basic code. \\footnote{See \\cite{srivastava2022beyond} for an encyclopedic list of examples.} Yet how this type of performance emerges from sequence predictions remains a subject of current debate.  Some have suggested that training on a sequence modeling task is inherently limiting. The arguments range from philosophical~\\citep{bender2020climbing} to mathematical~\\citep{merrill2021provable}. A common theme is that seemingly good performance might result from memorizing ``surface statistics,'' i.e., a long list of correlations that do not reflect a causal model of the process generating the sequence. This issue is of practical concern, since relying on spurious correlations may lead to problems on out-of-distribution data~\\citep{bender2021dangers, floridi2020gpt}.    On the other hand, some tantalizing clues suggest language models may do more than collect spurious correlations, instead building interpretable \\textit{world models}---that is, understandable models of the process producing the sequences they are trained on. Recent evidence suggests language models can develop internal representations for very simple concepts, such as color, direction~\\cite{abdou2021can, patel2022mapping}, or tracking boolean states during synthetic tasks~\\citep{li2021implicit} (see Related Work (\\autoref{sec:rw}) for more detail).  %The question remains, then, of how  we might investigate the emergence of world models in more complex domains.  A promising approach to studying the emergence of world models is used by \\cite{toshniwal2021learning}, which explores language models trained on chess move sequences. The idea is to analyze the behavior of a standard language modeling architecture in a well-understood, constrained setting. The paper finds that these models learn to predict legal chess moves with high accuracy. Furthermore, by analyzing predicted moves, the paper shows that the model appears to track the board state. The authors stop short, however, of exploring the form of any internal representations. Such an investigation will be the focus of this paper. A key motivation is the hope that ideas and techniques learned in this simplified setting may eventually be useful in natural-language settings as well.   \\subsection{The game of Othello as testbed for interpretability}  \\citet{toshniwal2021learning}'s  observations suggest a new approach to studying the representations learned by sequence models. If we think of a board as the ``world,'' then games provide us with an appealing experimental testbed to explore world representations of moderate complexity. As our setting, we choose the popular game of Othello (\\autoref{fig:othello_rules}), which is simpler than chess. This setting allows us to investigate world representations in a highly controlled context, where both the task and sequence being modeled are synthetic and well-understood.  %  As a first step, we train a language model (a GPT variant we call Othello-GPT) to extend partial game transcripts (a list of moves made by players) with legal moves. %\\ke{(\\textbf{not strategically good moves})}. \\textbf{The model has no a priori knowledge of the game or its rules}. All it sees during training is a series of tokens derived from the game transcripts. Each token represents a tile where players place their discs. Note that we do \\emph{not} explicitly train the model to make strategically good moves or to win the game. Nonetheless, our model is able to generate legal Othello moves with high accuracy.   Our next step is to look for world representations that might be used by the network. In Othello, the ``world'' consists of the current board position. A natural question is whether, within the model, we can identify a representation of the board state involved in producing its next move predictions. To study this question, we train a set of probes, i.e., classifiers which allow us to infer the board state from the internal network activations . This type of probing has become a standard tool for analyzing neural networks~\\citep{alain2016understanding,   tenney2019bert, belinkov2016probing}.   Using this probing methodology, we find evidence for an emergent world representation. In particular, we show that a non-linear probe is able to predict the board state with high accuracy (\\autoref{sec:probing}). (Linear probes, however, produce poor results.) This probe defines an internal representation of the board state. We then provide evidence that this representation plays a causal role in the network's predictions. Our main tool is an intervention technique that modifies internal activations so that they correspond to counterfactual board states.  We also discuss how knowledge of the internal world model can be used as an interpretability tool. Using our activation-intervention technique, we create \\textit{latent saliency maps}, which provide insight into how the network makes a given prediction. These maps are built by performing attribution at a high-level setting (the board) rather than a low-level one (individual input tokens or moves).   To sum up, we present four contributions: (1) we provide evidence for an emergent world model in a GPT variant trained to produce legal moves in Othello; (2) we compare the performance of linear and non-linear probing approaches, and find that non-linear probes are superior in this context; (3), we present an intervention technique that suggests that, in certain situations, the emergent world model can be used to control the network's behavior; and (4) we show how probes can be used to produce \\textit{latent saliency maps} to shed light on the model's predictions.   \\section{``Language modeling'' of Othello game transcripts}  Our approach for investigating internal representations of language models is to narrow our focus from natural language to a more controlled synthetic setting. We are partly inspired by the fact that language models show evidence of learning to make valid chess moves simply by observing game transcripts in training data~\\citep{toshniwal2021learning}. We choose the game Othello, which is simpler than chess, but maintains a sufficiently large game tree to avoid memorization. Our strategy is to see what, if anything, a GPT variant learns simply by observing game transcripts, without any a priori knowledge of rules or board structure.   \\begin{figure}[htbp] \\centering \\vspace{-2mm} \\includegraphics[width=0.8\\linewidth]{figs/othello_rules.pdf} %\\vspace{mm} \\caption{A visual explanation of Othello rules, from left to right: (A) The board is always initialized with four discs (two black, two white) placed in the center of the board. (B) Black always moves first. Every move must flip one or more opponent discs by outflanking---or sandwiching---the opponent disc(s). (C) The opponent repeats this process. A game ends when there are no more legal moves.} \\label{fig:othello_rules} \\vspace{-2mm} \\end{figure}  The game is played on an 8x8 board where two players alternate placing white or black discs on the board tiles. The object of the game is to have the majority of one's color discs on the board at the end of the game. Othello makes a natural testbed for studying emergent world representations since the game tree is far too large to memorize, but the rules and state are significantly simpler than chess.  The following subsections describe how we train a system with no prior knowledge of Othello to predict legal moves with high accuracy. The system itself is not our end goal; instead, it serves as our object of study.    \\subsection{Datasets: ``Championship'' and ``Synthetic''}  We use two sets of training data for the system, which we call ``championship'' and ``synthetic''. Each captures different objectives, namely data quality vs. quantity. While limited in size,  championship data reflects strategic moves by expert human players. The synthetic data set is far larger, consisting of legal but otherwise random moves.   Our \\emph{championship dataset} is produced by collecting Othello championship games from two online sources\\footnote{\\url{www.liveothello.com} and \\url{www.ffothello.org}.}, containing $7,605$ and $132,921$ games, respectively. They are combined and split randomly by $8:2$ into training and validation sets. The games in this dataset were produced by matches where human players presumably made moves with a strategic intent to win. Following this, we generate a \\emph{synthetic dataset} with $20$ million games for training and $3,796,010$ games for validation. We compute this dataset by uniformly sampling leaves from the Othello game tree. Its data distribution is different from the championship games, reflecting no strategy.   \\subsection{Model and Training}  Our goal is to study how much Othello-GPT can learn from pure sequence information, so we provide as few inductive biases as possible. (Note the contrast with a system like AlphaZero~\\citep{silver2018general}, where the goal was to win highly competitive chess games.) We therefore use only sequential tile indices as input to our model. For example, A4 and H6 are indexed as the $4$th and the $58$th word in our vocabulary, respectively. Each game is treated as a sentence tokenized with a vocabulary of 60 words, where each word corresponds to one of the $60$ tiles on which players put discs, excluding the $4$ tiles in the center (\\autoref{fig:othello_rules}).   We trained an 8-layer GPT model~\\citep{radford2018improving,radford2019language,brown2020language} with an 8-head attention mechanism and a 512-dimensional hidden space. The training was performed in an autoregressive fashion. For each partial game $\\{y_t\\}_{t=0}^{T-1}$, the computation process starts from indexing a trainable word embedding consisting of the 60 vectors, each for one word, to get $\\{x_t^0\\}_{t=0}^{T-1}$. They are then sequentially processed by $8$ multi-head attention layers. We denote the intermediate feature for the $t$-th token after the $l$-th layer as $x_t^l$. By employing a causal mask, only the features at the immediately preceding layer and earlier time steps $x_{\\leqslant t}^{l-1}$ are visible to $x_t^l$. Finally, $x_{T-1}^8$ goes through a linear classifier to predict logits for $\\hat{y}_{T}$. We minimize the cross-entropy loss between ground-truth move and predicted logits by gradient descent.   The model starts from randomly initialized weights, including in the word embedding layer. Though there are geometrical relationships between the 60 words (e.g., C4 is below B4), this inductive bias is not explicitly given to the model but rather left to be learned.   \\subsection{Othello-GPT Usually Predicts Legal Moves} We now evaluate how well the model's predictions adhere to the rules of Othello. For each game in the validation set, which was not seen during training, and for each step in the game, we ask Othello-GPT to predict the next legal move conditioned by the partial game before that move. We then calculate the error rate by checking if the top-$1$ prediction is legal. The error rate is $0.01\\%$ for Othello-GPT trained on the synthetic dataset and $5.17\\%$ for Othello-GPT trained on the championship dataset. For comparison, the untrained Othello-GPT has an error rate of $93.29\\%$. The main takeaway is that \\textbf{Othello-GPT does far better than chance in predicting legal moves} when trained on both datasets.  A potential explanation for these results may be that Othello-GPT is simply memorizing all possible transcripts. To test for this possibility, we created a \\emph{skewed dataset} of 20 million games to replace the training set of synthetic dataset. At the beginning of every game, there are four possible opening moves: C5, D6, E3 and F4. This means the lowest layer of the game tree (first move) has four nodes (the four possible opening moves). For our skewed dataset, we truncate one of these nodes (C5), which is equivalent to removing a quarter of the whole game tree. Othello-GPT trained on the skewed dataset still yields an error rate of $0.02\\%$. Since Othello-GPT has seen none of these test sequences before, pure sequence memorization cannot explain its performance~\\footnote{Note that even truncated the game tree may include some board states in the test dataset, since different move sequences can lead to the same board state. However, our goal is to prevent memorization of \\textit{input data}; the network only sees moves, and never sees board state directly.}.  If the performance of Othello-GPT is not due to memorization, what is it doing? We now turn to this question by probing for internal representations of the game state.  \\section{Exploring Internal Representations with Probes} \\label{sec:probing}  % background of probing for LM We seek to understand if Othello-GPT computes internal representations of the game state. One standard tool for this task is a ``probe''~\\citep{alain2016understanding, belinkov2016probing, tenney2019bert}. A probe is a classifier or regressor whose input consists of internal activations %$x_t^l$ from% of a network, and which is trained to predict a feature of interest, e.g., part of speech or parse tree depth~\\citep{hewitt2019structural}. If we are able to train an accurate probe, it suggests that a representation of the feature is encoded in the network's activations.  In our case, we want to know whether Othello-GPT's internal activations contain a representation of the current board state. To study this question, we train probes that predict the board state from the network's internal activations after a given sequence of moves. Note that the board state---whether each tile is empty or holds a black or white disc---is generally a nonlinear function of the input tokens. On the other hand, it is straightforward to write a program to compute this function, it makes a natural probe target.\\footnote{Classifying a tile as unoccupied or occupied can be written as a linear function of the input sequence, thus we consider only the $3$-way black/white/empty classifiers.}  We take the autoregressive features $x_t^l$ that summarize the partial sequence $y_{\\leqslant t}$ as the input to the probe and study results from different layers $l$. The output $p_\\theta(x_t^l)$ is a 3-way categorical probability distribution. We randomly split pairs of internal representation and ground-truth tile state by $8:2$ into training and validation set. Error rates on validation set are reported. A best random guess yields $52.95\\%$, if the probe always guess the tile is empty.  \\subsection{Linear Probes Have High Error Rates}  Our first result is that linear classifier probes have poor relative accuracy. Its function can be written as $p_\\theta(x_t^l)=\\text{softmax}(Wx_t^l)$ where $\\theta=\\{W \\in \\mathbb{R}^{F\\times 3} \\}$. $F$ is the number of dimensions of input $x_t^l$. As \\autoref{tab:linear_acc} shows,  error rates never dip below $20\\%$. As a baseline, we have included probes trained on a randomly initialized network\\footnote{Probes on the randomized network do better than chance; a constant guess of ``empty'' has a 47\\% error rate. But that performance is not a surprise, since it makes sense that some information about moves is preserved even by a random network. The key comparison is between the randomized network and the trained network.} We can see that there is only a marginal improvement in accuracy when we move to probing a fully-trained network. This result suggests that if there is an internal representation of the board state, it does not have a simple linear form.  \\begin{table}[h] \\center \\begin{tabular}{ccccccccc} \\hline              & $x^1$ & $x^2$ & $x^3$ & $x^4$ & $x^5$ & $x^6$ & $x^7$ & $x^8$ \\\\ \\cline{2-9}  Randomized   & 26.7 & 27.1 & 27.6 & 28.0 & 28.3 & 28.5 & 28.7 & 28.9 \\\\ Championship & 24.2 & 23.8 & 23.7 & 23.6 & 23.6 & 23.7 & 23.8 & 24.3 \\\\ Synthetic    & 21.9 & 20.5 & 20.4 & 20.6 & 21.1 & 21.6 & 22.2 & 23.1 \\\\ \\hline \\end{tabular} \\caption{Error rates ($\\%$) of linear probes on randomized Othello-GPT and Othello-GPTs trained on different datasets across different layers ($x^i$ represents internal representations after the $i$-th layer).} \\vspace{-4mm} \\label{tab:linear_acc} \\end{table}  \\subsection{Nonlinear Probes Have Lower Error Rates} \\label{nonlinear_probes}  Given the poor performance of linear probes, it is natural to ask whether a nonlinear probe would have higher accuracy. Moving up one notch of complexity, we apply a 2-layer MLP as a probe. This technique has been used successfully in other language model probing work, e.g., ~\\cite{conneau2018you,cao2021low,hernandez2021low}. Its function can be written as $p_\\theta(x_t^l)=\\text{softmax}(W_1 \\text{ReLU}(W_2 x_t^l))$ where $\\theta=\\{W_1 \\in \\mathbb{R}^{H\\times 3}, W_2 \\in \\mathbb{R}^{F\\times H}\\}$. $H$ is the number of hidden dimensions for the nonlinear probes.  The probe accuracy for trained networks, shown in \\autoref{tab:nonlinear_acc}, is significantly better than the linear probe in absolute terms. By contrast, the baseline (probing a randomized network with nonlinear probes) shows almost no improvement over the linear case. These results indicate that the probe may be recovering a nontrivial representation of board state in the network's activations. In \\autoref{sec:intvn}, we describe intervention experiments validating this hypothesis.  \\begin{table}[h] \\center \\begin{tabular}{ccccccccc} \\hline              & $x^1$ & $x^2$ & $x^3$ & $x^4$ & $x^5$ & $x^6$ & $x^7$ & $x^8$ \\\\ \\cline{2-9}  Randomized   & 25.5 & 25.4 & 25.5 & 25.8 & 26.0 & 26.2 & 26.2 & 26.4 \\\\ Championship & 12.8 & 10.3 & 9.5 & 9.4 & 9.8 & 10.5 & 11.4 & 12.4 \\\\ Synthetic    & 11.3 & 7.5 & 4.8 & 3.4 & 2.4 & 1.8 & 1.7 & 4.6 \\\\ \\hline \\end{tabular} \\caption{Error rates ($\\%$) of nonlinear probes on randomized Othello-GPT and Othello-GPTs trained on different datasets across different layers. Standard deviations are reported in \\autoref{app:probe_std}.} \\vspace{-5mm} \\label{tab:nonlinear_acc} \\end{table}  \\section{Validating Probes with Interventional Experiments} \\label{sec:intvn}  Our nonlinear probe accuracies suggest that  Othello-GPT computes information reflecting the board state. It's not obvious, however, whether that information is causal for the model's predictions.  %To investigate this issue, we evaluate whether the representations uncovered through \\autoref{sec:probing}  play a causal role in Othello-GPT's predictions.   In the following section, we adhere to ~\\citet{belinkov2016probing}'s recommendation, performing a set of interventional experiments to determine the causal relationship between model predictions and the emergent world representations.   To determine whether the board state information affects the network's predictions, we influence internal activations \\textit{during} Othello-GPT's calculation and measure the resulting effects. At a high level, the interventions are as follows: given a set of activations from the Othello-GPT, a probe predicts a baseline board state $B$. We record the move predictions associated with $B$, then modify these activations such that our probe reports an updated board state $B'$. Through our protocol, only a single tile $s$ distinguishes $B'$ from $B$'s board state (an example of which can be seen in \\autoref{fig:intvn}). This small modification results in a different set of possible legal moves for $B'$. If the new predictions match our expectations for $B'$---and not the predictions we recorded for $B$---we conclude the representation had a causal effect on the model.   \\begin{figure}[h] \\centering \\includegraphics[width=1\\linewidth]{figs/intervention_explanation.pdf}  \\caption{\\textbf{(A)} explains how we intervene on a board tile. Here, we only want to flip one tile, e.g. E6, from white to black. In \\textbf{(B)}, four views present an Othello game in progression, which can be reliably probed from an internal representation $x$. The lower left board represents the model's perceived world state prior to intervention. The upper left board shows the model's predictions for legal moves given this state. Post-intervention, the model's world state is updated---E6's state has been switched from white to black (lower right), leading to a different set of legal move predictions (upper right). Note that two tiles (E6) are highlighted in the world state boards.  This is the tile that we ``intervene'' on, changing from white to black. \\textbf{(C)} Shows our proposed intervention scheme. Light blue indicates unmodified activations; dark blue represents activations affected by intervention. Starting from a predefined layer, we intervene at the temporally-last token (shown in \\textbf{(A)}). We replace original internal representations with the post-intervention one and resume computation for the next one layer. Part of the misinformation gets corrected (light blue), but we alternate this intervening and computation process until the last layer, from which the next-step prediction is made.} \\vspace{-5mm} \\label{fig:intvn} \\end{figure}  \\subsection{Intervention Technique}  To implement an intervention that changes the predicted state from a board position $B$ to a modified version $B'$ we must decide (a) which layers to modify activations in, and (b) how to modify those activations. The first question is subtle. Given the causal attention mechanism of GPT-2, modifying activations for only one layer is unlikely to be effective as later layer computations incorporate information from prior board representations unaffected by our intervention. Instead, we select an initial layer $L_s$ then modify it and subsequent layers' activations (see \\autoref{fig:intvn} (C)). %Critical to this experimental procedure, we must modify the internal activations at a given layer to change the predicted board state.  Our modification uses a simple gradient descent method on the probe's class score for the particular tile $s$ whose state is being modified.  \\autoref{fig:intvn} illustrates an intervention on a single feature $x$ into $x'$ such that the corresponding board state $B$ is updated to match the desired $B'$. We observe the effectiveness of these interventions by probing the intervened $x'$ or $x$ at later layers (see \\autoref{app:probe_interaction}), as well as the change in next-step prediction (see \\autoref{sec:intvn_res}). Consistent with the training process of probes $p_\\theta$, we use cross entropy loss between the probe-predicted probability distribution and the desired board state, but rather than optimize probe weights $\\theta$, we optimize $x$ for intervention \\footnote{Note that $\\alpha$ is the learning rate. See more on the intervention hyper-parameters in \\autoref{app:intervention_regularization}.}:  \\begin{align*}     x' \\leftarrow x - \\alpha \\frac{\\partial \\mathcal{L}_\\text{CE}(p_\\theta(x), B')}{\\partial x}. \\end{align*} % At timestep $T$, the internal activations of an $L$-layer Othello-GPT can be viewed as an $L \\times T$ grid of activation vectors. Our intervention process runs Othello-GPT sequentially, but uses gradient descent to modify key activation vectors at the last timestep such that their board state class scores change. Note that if we change activations only at a middle layer, activations at higher layers are directly affected by pre-intervention information. Thus, we sequentially intervene $\\{ x_{T-1}^l \\}_{l=L_s}^L$ at the last timestep, on all activations starting from a preset layer $L_s$ until the final layer, illustrated in \\autoref{fig:intvn}.  \\subsection{Evidence for a Causal Role for the Representation} \\label{sec:intvn_res}  To systematically determine if this world representation is causal for model predictions, we create an evaluation benchmark set. A test case in this set consists of a triple of a partial game, a targeted board tile, and a target state. For each case, we give the partial game to Othello-GPT and perform the intervention described in the previous section. I.e., we extract the model's activations mid-computation, modify them to change the representation of the targeted board tile into the target state, give back the modified world representation and let it make a prediction with this new world state.   %\\kl{KL revised this.} \\mw{MW edited too} The benchmark set consists of two subsets of $1000$ intervention cases: one ``natural,'' one ``unnatural.'' The natural subset consists of positions reachable by legal play. The unnatural subset contains position that are unreachable by legal play. This second subset is designed to be a stringent test, since it is by definition far from anything encountered in the training distribution.  We measure prediction alignment with the intervened world representation as a multi-label classification problem, comparing the top-$N$ predictions against the ground-truth legal next-move set, where $N$ is the number of legal next-moves after intervention. We then calculate an error per case (a sum of false positives and false negatives, shown in \\autoref{fig:intvn_res})\\footnote{Qualitative results can be found in \\autoref{app:prediction_heatmaps}. We report additional metrics in \\autoref{app:metrics}.}. For both benchmarks, $L_s=4$ (intervening $5$ layers) gives the best result: average errors of $\\boldsymbol{0.12}$ and $\\boldsymbol{0.06}$ respectively. Compared to baseline errors ($\\boldsymbol{2.68}$ and $\\boldsymbol{2.59}$), the proposed intervention technique is effective even under unnatural board states, suggesting the emergent representations are causal to model predictions.   \\begin{figure}[t] \\centering \\includegraphics[width=.85\\linewidth]{figs/intvn_res_err.pdf} \\vspace{-2mm} \\caption{Intervention experiment results. Red dashed lines represents average number of errors by testing pre-intervention predictions on post-intervention ground-truths, representing a null intervention method for contrast. The shaded area represents the $95\\%$ confidence interval.} \\label{fig:intvn_res} \\vspace{-5mm} \\end{figure}  \\section{Latent Saliency Maps: Attribution via Intervention} \\label{sec:avi}  The intervention technique of the previous section provides insight into the predictions of Othello-GPT. We can also use it to create visualizations which contextualize Othello-GPT's predictions in terms of the board state. The basic idea is simple. For each tile $s$ on the board $B$, we ask how much the network's prediction probability for the attributed tile $\\textnormal{p}$ will change if we apply the intervention technique in \\autoref{sec:intvn} to change the state representation of that tile $s$. This will yield a value per tile $s \\in B$, positive or negative, corresponding to its saliency in the prediction of $\\textnormal{p}$ (see \\autoref{algo:attribution}). We then create a visualization of the board where tiles are colored according to their saliency for the top-1 prediction for the current board state\\footnote{However, ideally we can attribute any prediction. See \\autoref{app:vis_discuss} for more discussion.}. Because this map is based on the network's latent space rather than its input, we call it a \\textbf{latent saliency map}.   \\begin{figure}[h] \\centering \\includegraphics[width=1\\linewidth]{figs/attribution.pdf} \\caption{Latent saliency maps: Each subplot shows a different game state, and the top-1 prediction by the model is enclosed in a black box. Colors (red is high, blue is low) indicate the contribution of a square's state to this prediction. The contribution is higher when changing the internal representation of this square makes the prediction less likely. The values are normalized by subtracting the mean of the board. \\textbf{(A)} Latent saliency maps for Othello-GPT trained on the synthetic dataset, where the model learns \\textit{legal} moves. \\textbf{(B)} Latent saliency maps for Othello-GPT trained on the championship dataset. Rather than learning rules, this Othello-GPT learns to make \\emph{strategically good} moves.} \\vspace{-6mm} \\label{fig:avi} \\end{figure}  \\autoref{fig:avi} shows latent saliency maps for the top-1 predictions for Othello-GPTs trained on the synthetic and championship datasets by intervening from $L_s=4$. The two diagrams show a clear pattern. The synthetic Othello-GPT shows high saliency for precisely those tiles that are required to make a move legal. In almost all cases, other tiles have lower saliency values. Even without knowing how synthetic-GPT was trained, an experienced Othello player might be able to guess its goal. The latent saliency maps for the championship version, however, are more complex. Although tiles that relate directly to legality typically have high values, many other tiles show high saliency as well. This pattern makes sense, too. Expert moves rely on complex global features of the board. The difference between the latent saliency maps for the two versions of Othello-GPT suggests that the visualization technique is providing useful information about the two systems.   \\begin{algorithm}[!ht] \\caption{Attribution via Intervention on Othello-GPT} \\label{algo:attribution} \\DontPrintSemicolon % \\SetAlgoLined \\SetKwInOut{Input}{Inputs} \\SetKwInOut{Output}{Output} \\SetInd{0.5em}{0.5em} % \\Indm \\Input{ \\par \\begin{tabular}{l l} $B$ & the current board state \\\\ $\\textnormal{p}$ & a legal next move which we try to attribute \\\\ \\end{tabular} } \\Output{ \\par \\begin{tabular}{l l} $\\{ S_s \\}_{s \\in B}$ & assigned sensitivity values for the prediction of $\\textnormal{p}$ \\end{tabular} }  $p_0 \\gets f_\\textnormal{p}(x_{t-1}) $ \\tcp*{pre-intervention next-step probability for $\\textnormal{p}$}  \\For{$s \\in B$}{ %\\textnormal{occupied tile}     $\\tilde{x}_{t-1} \\gets \\operatorname{Intervention}(x_{t-1}, s) $ \\\\      $p_s \\gets f_\\textnormal{p}(\\tilde{x}_{t-1}) $ \\\\     $S_s \\gets p_0 - p_s$ }  \\end{algorithm} \\vspace{-3mm} \\section{Related work} \\label{sec:rw}   Our work fits into a general line of investigation into world representations created by sequence models. For example, \\citep{li2021implicit} fine-tune two language models on synthetic natural language tasks~\\citep{long2016simpler} and find evidence that semantic information about the underlying world state is at least weakly encoded in the activations of the network. More direct evidence of a faithful representation of 3D color space comes from \\cite{abdou2021can}, who examine activations in the BERT model and find a geometric connection to a standard 3D color space. Another study by~\\citep{patel2022mapping} shows that language models can learn to map conceptual domains, e.g., direction and color, onto a grounded world representation via prompting techniques~\\citep{brown2020language}. These investigations operate in natural language domains, but investigate relatively simple world models.  Another related stream of work concerns neural networks that learn board games. There is a long history of work in AI to learn game moves, but in general, these systems have been given some a priori knowledge of the structure of the game. Even one of the most general-purpose game-playing engines, AlphaZero~\\citep{silver2018general}, has built-in knowledge of basic board structure and game rules (although, intriguingly, it seems to develop interpretable models of various strategic concepts~\\citep{mcgrath2021acquisition, forde2022concepts}). Closer to the work described here--and a major motivation for our research--is \\cite{toshniwal2021learning} which trains a language model on chess transcripts. They show strong evidence that transformer networks are building a representation of internal board state, but they stop short at investigating what form that representation takes.% Our work can be seen as building on this line of research, with a focus on the geometry of internal representations (\\autoref{sec:geo}).  The intervention technique of \\autoref{sec:intvn} follows an approach of steering model output while keeping the model frozen. It is related to the ideas behind plug-and-play controllable text generation for autoregressive~\\citep{dathathri2019plug,qin2020back,krause2020gedi} and diffusion~\\citep{li2022diffusion} language models by optimizing the likelihood of the desired attribute and the fluency of generated texts at the same time. These methods naturally involve a trade-off and require several forward and backward passes to generate. Our proposed intervention method stands out by only working on internal representations and requires only one forward pass. Finally, latent saliency maps can be viewed as a generalization of the TCAV  approach~\\cite{kim2018interpretability,ghorbani2019towards,koh2020concept}. In the TCAV setting, attribution is performed via directional derivatives. This is essentially a linearization of the gradient-descent optimization used in our attribution maps. % \\section{Conclusion} % Our experiments provide evidence that Othello-GPT maintains a representation of game board states---that is, the Othello ``world''---to produce sequences it was trained on. This representation appears to be nonlinear in an essential way. Further, we find that these representations can be causally linked to how the model makes its predictions. Understanding of the internal representations of a sequence model is interesting in its own right, but may also be helpful in deeper interpretations of the network.   We have also described how interventional experiments may be used to create a ``latent saliency map'',  which gives a picture, in terms of the Othello board, of how the network has made a prediction. Applied to two versions of Othello-GPT that were trained on different data sets, the latent saliency maps highlight the dramatic differences between underlying representations of the Othello-GPT trained on synthetic dataset and its counterpart trained on championship dataset.  There are several potential lines of future work. One natural extension would be to perform the same type of investigations with other, more complex games. It would also be interesting to compare the strategies learned by a sequence model trained on game transcripts with those of a model trained with a priori knowledge of Othello. One option is to compare latent saliency maps of Othello--GPT with standard saliency maps of an Othello-playing program which has the actual board state as input.   More broadly, it would be interesting to study how our results generalize to models trained on natural language. One stepping stone might be to look at language models whose training data has included game transcripts. Will we see similar representation of board state? Grammar engineering tools~\\citep{weston2015towards,hermann2017grounded,cote2018textworld} could help define a synthetic data generation process that maps world representations onto natural language sentences, providing a similarly controllable setting like Othello while closing the distance to natural languages. For more complex natural language tasks, can we find meaningful world representations? Our hope is that the tools described in this paper---nonlinear probes, layerwise interventions, and latent saliency maps---may prove useful in natural language settings.  \\clearpage \\subsubsection*{Acknowledgments} We thank members of the Visual Computing Group and the Insight + Interaction Lab at Harvard for their early discussions and feedback. We especially thank Aoyu Wu for helping with making part of the figures and other valuable suggestions. We gratefully acknowledge the support of Harvard SEAS Fellowship (to KL), Siebel Fellowship (to AH), Open Philanthropy (to DB), and FTX Future Fund Regrant Program (to DB). This work was partially supported by NSF grant IIS-1901030.  \\bibliography{iclr2023_conference} \\bibliographystyle{iclr2023_conference}  \\clearpage \\appendix \\section{Visualizing the Geometry of Probes} \\label{sec:geo}  \\begin{figure}[h] \\centering \\includegraphics[width=0.8\\linewidth]{figs/geometry_probe.jpg} \\caption{Geometry of probe weights. The first row is for linear probe weights; the second row is for the second-layer weights of the nonlinear probes. Three columns from left to right mean: probes trained on randomly-initialized GPT, on GPTs trained on championship and synthetic datasets respectively. For each subplot, one dot represents one tile on an Othello board, and neighbouring tiles have their two dots connected. Top-left and right-bottom corners are labelled for reference. Togglable 3D plots can be found in Supplementary Material.} \\label{fig:probegeo} \\end{figure}  Both linear and nonlinear probes can be viewed as geometric objects. In the case of linear probes, we can associate each classifier with the normal vector to the separating hyperplane. In the case of nonlinear probes, we can treat the second layer of the MLP as a linear classifier and take the same view. This perspective associates a vector to each grid tile, corresponding to the classifier for that grid tile.  A natural question is whether these vectors display any geometric patterns. \\autoref{fig:probegeo} visualizes their configuration using PCA plots. To make the connection with board geometry clear, we have overlaid a grid in which the vector for a given grid tile is connected to the vectors that represent its neighbors. At left, as a baseline, are weights of probes trained on randomized GPTs; the result is a somewhat jumbled version of the board's grid. For classifier vectors, however, we see a somewhat clearer geometric correlation with the grid shape itself. This shape may reflect a set of correlations between neighboring grid cells, and could be an interesting object for future study. One point of interest is that probe geometry for the randomized network does not seem completely random. This may fit with the fact that linear probe baseline performance is better than chance, indicating some information about board state can be gleaned from random projections of game move tokens.  \\section{Ablation on Nonlinear Probe Accuracies}  Though high accuracies have been observed on nonlinear probes in \\autoref{sec:probing}, we want to develop a deeper understanding on them. For example, we wish to understand when, during a game, a model has developed world representations of board tiles, where in Othello-GPT that information is stored, how difficult it is to decode that information, and when the model may forget that information.  As shown in \\autoref{fig:whw} (B), we plot probe accuracies of two-layer probes varying to two different experiment settings: (1) Probe Hidden Units: the number of hidden units $H$ in nonlinear probes; (2) Layer: at which layer the representations $x^l$ is taken out. With the increase of hidden units, i.e., probe capacity, probe accuracy is higher as it can capture more knowledge from the hidden space. For layer $l$, the accuracy peaks at midway, which is aligned with studies on natural language~\\citep{hewitt2019structural}, where linguistic properties are found to be best probed in mid-layers. The format of our \\textit{what-how-where} plots is similar to the \\textit{what-when-where} plots in \\cite{mcgrath2021acquisition}, except we ask how many hidden units within our nonlinear probe are necessary to achieve reasonable accuracy given each layer of the GPT instead of looking into number of training epochs.   \\begin{figure}[t] \\centering \\includegraphics[width=\\linewidth]{figs/whw.pdf} %\\vspace{mm} \\caption{(A) \\textit{What-how-where} plots and (B) \\textit{What-how-when} plots of nonlinear probes trained on randomly-initialized GPT, on GPTs trained on championship and synthetic datasets respectively. (A) presents probe accuracy across an Othello game progression, while (B) presents accuracy across Othello-GPT layers.} \\label{fig:whw} \\end{figure}  We are also curious about when (in terms of game progression) these concepts are captured by Othello-GPT. Are these concepts updated immediately after each move? Will they persist or be forgotten with newer moves being made? To study this, we divide the data points for probe validation by how many steps the tile has been in its current state and plot \\emph{what} concept can be probed by \\emph{how} powerful probes \\emph{when} in the game progression in \\autoref{fig:whw} (A).  For nonlinear probes with a moderate number of hidden units, a parabolic accuracy curve is shown: concepts are best captured when they have existed for some time but not too long. This tells us: (1) forgetting happens when Othello-GPT changes its world representations; (2) there is a period of uncertainty before changes in board states are updated.  \\section{Prediction heatmaps of counterfactual board states} \\label{app:prediction_heatmaps}  In \\autoref{fig:appa}, we can see one case of how intervention changes model prediction by intervening on the world representation of Othello-GPT. Note that the set of ground-truth legal moves are also changed by the intervention. In this case, both pre-intervention and post-intervention predictions have $0$ errors. \\autoref{fig:intvn_res} shows systematic results over $1,000$ cases. A random subset of $100$ cases can be found in Supplementary Materials.  \\begin{figure}[h]     \\centering     \\includegraphics[width=0.75\\linewidth]{figs/AppendixA_101.pdf}     \\caption{Heatmaps of the probability allocated to each tile on the board with nonlinear intervention done on Othello-GPT trained on synthetic dataset. To the left is the pre-intervention heatmap with pre-intervention legal moves underscored; to the right is post-intervention heatmap, with post-intervention legal moves underscored. }     \\label{fig:appa} \\end{figure}   \\section{Inter-probe Interaction} \\label{app:probe_interaction}  In \\autoref{fig:appb} we show the same case as in \\autoref{app:prediction_heatmaps} of the world states probed from layers of Othello-GPT starting the $5$th, which is the layer we start to intervene, $L_s$. We can observe that after intervention is successfully done on the $5$th layer at C4, the flipped disc is corrected in the immediately following layer, as seen in the pre-intervention probing results at the $6$-th layer. However, when the same intervention is done on later layers again, the model is more convinced that C4 should be black and stops to correct it. A total of $100$ cases can be found in Supplementary Materials, corresponding to examples in \\autoref{app:prediction_heatmaps}.   \\begin{figure}[h!]     \\centering     \\includegraphics[width=1\\linewidth]{figs/AppendixB_101.pdf}     \\caption{The world states predicted by probes pre-and-post-intervention on internal representations at different layers. Color encodes the confidence of the plotted top-$1$ prediction for each tile.}     \\label{fig:appb} \\end{figure}  \\section{Discussion of Latent Saliency Maps} \\label{app:vis_discuss}  In the latent saliency maps created by attribution via intervention in \\autoref{fig:avi}, when multiple discs are flipped, only the first one are found contributing, which is slighted misaligned to human understanding of the Othello rule. However, this is expected from the algorithm we are using because other flipped discs, even in the opposite states, still make the current prediction legal.  The attribution via intervention method succeeds at visualizing the AND-logic in the Othello rule to flip one straight line: there should be opponent discs in between \\emph{and} same-color disc at the other end. However, when the prediction flips more than one straight lines, intervening on one of them does not nullify the prediction. This is because the Othello rule is an OR-logic on top of the AND-logic: a move is legal when either one of the eight straight lines can be flipped. How to extract such knowledge with unlimited amount of intervention experiments at hand? We leave it for future research.   \\section{Alternative Metrics for the Intervention Experiment} \\label{app:metrics}  \\begin{figure}[t] \\centering \\includegraphics[width=.85\\linewidth]{figs/intvn_res_f1.pdf} \\vspace{-2mm} \\caption{Same as \\autoref{fig:intvn_res} except for reporting $F_1$ instead of Error.} \\label{fig:intvn_res_f1}  \\includegraphics[width=.85\\linewidth]{figs/intvn_res_kl.pdf} \\vspace{-2mm} \\caption{Same as above but for $D_{KL}$.} \\label{fig:intvn_res_kl} \\vspace{-5mm} \\end{figure}  Following the same multi-label classification framework that calculates the error in \\autoref{fig:intvn_res}, we report the $F_1$ score in \\autoref{fig:intvn_res_f1}. We measure intervention success as a multi-class classification problem, rather than a top-$1$ prediction problem like in \\autoref{sec:probing}, considering the fact that the later metric is almost saturated for the Othello-GPT trained on synthetic dataset and the intervention in benchmark only cause a set difference of $2.12$ tiles on average.  Another line of thought to measure the alignment between model prediction and the intervened world representation is by measuring the KL divergence of the predicted next-move probability distribution from a discrete uniform prior distribution over all legal next-moves. Note that the baseline $F_1$ and $D_{KL}$ are the averages of the $F_1$ and $D_{KL}$ over all $1000$ intervention cases, rather than treating the whole benchmark as one single multi-label classification problem or distribution distance calculation.  Error (\\autoref{fig:intvn_res}), $F_1$ (\\autoref{fig:intvn_res_f1}), and KL divergence (\\autoref{fig:intvn_res_kl}) show a similar trend, confirming that the proposed intervention technique is useful and validating the causality from world representations to model predictions.   \\section{Ablations on Intervention Hyper-parameters} \\label{app:intervention_regularization}  Experiments find this optimization process is robust to different configurations of optimizer, learning rate $\\alpha$, and number of steps.   The complete world state, including states of all the $64$ board tiles, is encoded in a single internal representation $x$, while during intervention experiment, we only wish to change one of them. A natural question is: will the intervention operation flip tiles we are not not intending? It is possible but we can mitigate that by considering the cross entropy losses of other tiles as a regularization term, weighted by a hyper-parameter $\\beta$. That is to say, the loss in main paper can be expanded as: \\begin{align*}     \\mathcal{L}_\\text{CE}(p_\\theta(x), B') = \\sum_{s \\in B \\neq B'} \\mathcal{L}_\\text{CE}(p^s_{\\theta}(x), B'(s)) + \\beta\\sum_{s \\in B = B'} \\mathcal{L}_\\text{CE}(p^s_{\\theta}(x), B(s)). \\end{align*} We sweep $\\beta$ at the best $L_s=5$ on the natural benchmark and report average number of errors in \\autoref{tab:sweep_beta}. We can observe it does not clearly help.   \\begin{table}[h!] \\center \\begin{tabular}{rcccccccccc} \\hline $\\beta$ & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 \\\\ \\cline{2-11}  \\text{error} & 0.22 & 0.31 & 0.77 & 0.19 & 0.51 & 0.65 & 0.42 & 0.45 & 0.46 & 0.55 \\\\ \\hline \\end{tabular} \\caption{Average number of errors under different $\\beta$'s and $L_s=5$ on the natural benchmark.} \\label{tab:sweep_beta} \\end{table}  Here we further discuss another hyper-parameter ablated in Figure~\\ref{fig:intvn_res}, the starting layer for intervention, $L_s$. If we intervene with more than $5$ layers, shallow layers which have not developed reliable world representations (according to \\autoref{fig:whw}) are touched, making intervention hazardous. On the other hand, if we intervene only at the deepest layers, though the world representation can be intervened successfully (see \\autoref{app:probe_interaction}), the model does not have enough computation to adapt to the newer world representation and make predictions corresponding to it.   \\section{Standard Deviation of Probe Accuracy} \\label{app:probe_std}  In order to exclude the possibility that the differences in probe accuracies in \\autoref{tab:linear_acc} and \\autoref{tab:nonlinear_acc} are caused by randomness, we run the same experiment for $100$ times with different random seeds and report the standard deviations of them in \\autoref{tab:linear_var} and \\autoref{tab:nonlinear_var}, based on which we conclude that the probe accuracies are robust to randomness and there are significant differences between the probe accuracies of linear and nonlinear probes, and also between probes on randomized Othello-GPT and Othello-GPTs trained on different datasets.  \\begin{table}[h] \\center \\begin{tabular}{ccccccccc} \\hline              & $x^1$ & $x^2$ & $x^3$ & $x^4$ & $x^5$ & $x^6$ & $x^7$ & $x^8$ \\\\ \\cline{2-9}  Randomized   & 0.04 & 0.04 & 0.05 & 0.06 & 0.06 & 0.06 & 0.09 & 0.06 \\\\ Championship & 0.05 & 0.04 & 0.06 & 0.07 & 0.06 & 0.06 & 0.06 & 0.06 \\\\ Synthetic    & 0.06 & 0.06 & 0.07 & 0.06 & 0.05 & 0.04 & 0.06 & 0.08 \\\\ \\hline \\end{tabular} \\caption{Standard deviations of error rates ($\\%$) of linear probes on randomized Othello-GPT and Othello-GPTs trained on different datasets across different layers ($x^i$ represents internal representations after the $i$-th layer).} \\label{tab:linear_var} \\end{table}  \\begin{table}[h] \\center \\begin{tabular}{ccccccccc} \\hline              & $x^1$ & $x^2$ & $x^3$ & $x^4$ & $x^5$ & $x^6$ & $x^7$ & $x^8$ \\\\ \\cline{2-9}  Randomized   & 0.05 & 0.06 & 0.07 & 0.04 & 0.04 & 0.03 & 0.04 & 0.03 \\\\ Championship & 0.05 & 0.09 & 0.08 & 0.06 & 0.05 & 0.05 & 0.10 & 0.06 \\\\ Synthetic    & 0.07 & 0.09 & 0.04 & 0.06 & 0.06 & 0.09 & 0.05 & 0.05 \\\\ \\hline \\end{tabular} \\caption{Standard deviations of error rates ($\\%$) of nonlinear probes on randomized Othello-GPT and Othello-GPTs trained on different datasets across different layers ($x^i$ represents internal representations after the $i$-th layer).} \\label{tab:nonlinear_var} \\end{table}  \\end{document}\n"
     ]
    }
   ],
   "source": [
    "chonk = input(\"paste raw LaTeX here:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb976a8",
   "metadata": {},
   "source": [
    "## just text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49c0d7a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T21:30:37.693892Z",
     "start_time": "2024-03-02T21:28:33.385749Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [02:04<00:00,  5.18s/it]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "\n",
    "sections = split_latex_by_section(chonk)\n",
    "for i, section in enumerate(tqdm(sections)):\n",
    "    # Generate the chat completion\n",
    "    snippet = generate_snippet(section)\n",
    "    \n",
    "    # Check if the snippet is too long\n",
    "    if len(snippet) > 4096:\n",
    "        # Split the snippet at an arbitrary newline\n",
    "        split_index = snippet.find('\\n', len(snippet) // 4)\n",
    "        snippet_part1 = snippet[:split_index]\n",
    "        snippet_part2 = snippet[split_index + 1:]\n",
    "        \n",
    "        # rerun cleaning for both parts\n",
    "        snippet_part1 = generate_snippet(snippet_part1)\n",
    "        snippet_part2 = generate_snippet(snippet_part2)\n",
    "        \n",
    "        text += f\"{snippet_part1} \\n\\n\"\n",
    "        text += f\"{snippet_part2} \\n\\n\"\n",
    "    else:\n",
    "        # Generate audio for the snippet\n",
    "        text += f\"{snippet} \\n\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb765824",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T21:30:57.447700Z",
     "start_time": "2024-03-02T21:30:57.442971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction. Recent language models have shown an intriguing range of capabilities. Networks trained on a simple \"next-word\" prediction task are apparently capable of many other things, such as solving logic puzzles or writing basic code. \n",
      "\n",
      "Yet how this type of performance emerges from sequence predictions remains a subject of current debate. Some have suggested that training on a sequence modeling task is inherently limiting. The arguments range from philosophical (Bender, 2020) to mathematical (Merrill, 2021). A common theme is that seemingly good performance might result from memorizing \"surface statistics,\" i.e., a long list of correlations that do not reflect a causal model of the process generating the sequence. This issue is of practical concern, since relying on spurious correlations may lead to problems on out-of-distribution data (Bender, 2021; Floridi, 2020).\n",
      "\n",
      "On the other hand, some tantalizing clues suggest language models may do more than collect spurious correlations, instead building interpretable world models---that is, understandable models of the process producing the sequences they are trained on. Recent evidence suggests language models can develop internal representations for very simple concepts, such as color, direction (Abdou et al., 2021; Patel et al., 2022), or tracking boolean states during synthetic tasks (Li et al., 2021) (see Related Work (Section 2) for more detail).\n",
      "\n",
      "A promising approach to studying the emergence of world models is used by Toshniwal et al. (2021), which explores language models trained on chess move sequences. The idea is to analyze the behavior of a standard language modeling architecture in a well-understood, constrained setting. The paper finds that these models learn to predict legal chess moves with high accuracy. Furthermore, by analyzing predicted moves, the paper shows that the model appears to track the board state. The authors stop short, however, of exploring the form of any internal representations. Such an investigation will be the focus of this paper. A key motivation is the hope that ideas and techniques learned in this simplified setting may eventually be useful in natural-language settings as well. \n",
      "\n",
      "The game of Othello as a testbed for interpretability. Toshniwal et al.'s observations suggest a new approach to studying the representations learned by sequence models. If we think of a board as the \"world,\" then games provide us with an appealing experimental testbed to explore world representations of moderate complexity. As our setting, we choose the popular game of Othello, which is simpler than chess. This setting allows us to investigate world representations in a highly controlled context, where both the task and sequence being modeled are synthetic and well-understood.\n",
      "\n",
      "As a first step, we train a language model (a GPT variant we call Othello-GPT) to extend partial game transcripts (a list of moves made by players) with legal moves. The model has no a priori knowledge of the game or its rules. All it sees during training is a series of tokens derived from the game transcripts. Each token represents a tile where players place their discs. Note that we do not explicitly train the model to make strategically good moves or to win the game. Nonetheless, our model is able to generate legal Othello moves with high accuracy.\n",
      "\n",
      "Our next step is to look for world representations that might be used by the network. In Othello, the \"world\" consists of the current board position. A natural question is whether, within the model, we can identify a representation of the board state involved in producing its next move predictions. To study this question, we train a set of probes, i.e., classifiers which allow us to infer the board state from the internal network activations. This type of probing has become a standard tool for analyzing neural networks. Using this probing methodology, we find evidence for an emergent world representation. In particular, we show that a non-linear probe is able to predict the board state with high accuracy.\n",
      "\n",
      "Linear probes, however, produce poor results. This probe defines an internal representation of the board state. We then provide evidence that this representation plays a causal role in the network's predictions. Our main tool is an intervention technique that modifies internal activations so that they correspond to counterfactual board states. We also discuss how knowledge of the internal world model can be used as an interpretability tool.\n",
      "\n",
      "Using our activation-intervention technique, we create latent saliency maps, which provide insight into how the network makes a given prediction. These maps are built by performing attribution at a high-level setting (the board) rather than a low-level one (individual input tokens or moves).\n",
      "\n",
      "To sum up, we present four contributions: (1) we provide evidence for an emergent world model in a GPT variant trained to produce legal moves in Othello; (2) we compare the performance of linear and non-linear probing approaches, and find that non-linear probes are superior in this context; (3) we present an intervention technique that suggests that, in certain situations, the emergent world model can be used to control the network's behavior; and (4) we show how probes can be used to produce latent saliency maps to shed light on the model's predictions. \n",
      "\n",
      "Section \"Language modeling\" of Othello game transcripts. Our approach for investigating internal representations of language models is to narrow our focus from natural language to a more controlled synthetic setting. We are partly inspired by the fact that language models show evidence of learning to make valid chess moves simply by observing game transcripts in training data (Toshniwal et al., 2021). We choose the game Othello, which is simpler than chess, but maintains a sufficiently large game tree to avoid memorization. Our strategy is to see what, if anything, a GPT variant learns simply by observing game transcripts, without any a priori knowledge of rules or board structure.\n",
      "\n",
      "The game is played on an 8x8 board where two players alternate placing white or black discs on the board tiles. The object of the game is to have the majority of one's color discs on the board at the end of the game. Othello makes a natural testbed for studying emergent world representations since the game tree is far too large to memorize, but the rules and state are significantly simpler than chess. The following subsections describe how we train a system with no prior knowledge of Othello to predict legal moves with high accuracy. The system itself is not our end goal; instead, it serves as our object of study. \n",
      "\n",
      "Subsection: Datasets: \"Championship\" and \"Synthetic\"\n",
      "\n",
      "We use two sets of training data for the system, which we call \"championship\" and \"synthetic\". Each captures different objectives, namely data quality versus quantity. While limited in size, championship data reflects strategic moves by expert human players. The synthetic dataset is far larger, consisting of legal but otherwise random moves. Our championship dataset is produced by collecting Othello championship games from two online sources [1], [2], containing 7,605 and 132,921 games, respectively. They are combined and split randomly by 8:2 into training and validation sets. The games in this dataset were produced by matches where human players presumably made moves with a strategic intent to win. Following this, we generate a synthetic dataset with 20 million games for training and 3,796,010 games for validation. We compute this dataset by uniformly sampling leaves from the Othello game tree. Its data distribution is different from the championship games, reflecting no strategy. \n",
      "\n",
      "Model and Training\n",
      "\n",
      "Our goal is to study how much Othello-GPT can learn from pure sequence information, so we provide as few inductive biases as possible. (Note the contrast with a system like AlphaZero (Silver et al., 2018), where the goal was to win highly competitive chess games.) We therefore use only sequential tile indices as input to our model. For example, A4 and H6 are indexed as the 4th and the 58th word in our vocabulary, respectively. Each game is treated as a sentence tokenized with a vocabulary of 60 words, where each word corresponds to one of the 60 tiles on which players put discs, excluding the 4 tiles in the center. \n",
      "\n",
      "We trained an 8-layer GPT model (Radford et al., 2018; Radford et al., 2019; Brown et al., 2020) with an 8-head attention mechanism and a 512-dimensional hidden space. The training was performed in an autoregressive fashion. For each partial game {yt} from 0 to T-1, the computation process starts from indexing a trainable word embedding consisting of the 60 vectors, each for one word, to get {xt^0} from 0 to T-1. They are then sequentially processed by 8 multi-head attention layers. We denote the intermediate feature for the t-th token after the l-th layer as xt^l. By employing a causal mask, only the features at the immediately preceding layer and earlier time steps xâ‰¤t^l-1 are visible to xt^l. Finally, xT-1^8 goes through a linear classifier to predict logits for y^T. We minimize the cross-entropy loss between the ground-truth move and predicted logits by gradient descent. \n",
      "\n",
      "The model starts from randomly initialized weights, including in the word embedding layer. Though there are geometrical relationships between the 60 words (e.g., C4 is below B4), this inductive bias is not explicitly given to the model but rather left to be learned. \n",
      "\n",
      "Subsection Othello-GPT Usually Predicts Legal Moves: We now evaluate how well the model's predictions adhere to the rules of Othello. For each game in the validation set, which was not seen during training, and for each step in the game, we ask Othello-GPT to predict the next legal move conditioned by the partial game before that move. We then calculate the error rate by checking if the top-1 prediction is legal. The error rate is 0.01% for Othello-GPT trained on the synthetic dataset and 5.17% for Othello-GPT trained on the championship dataset. For comparison, the untrained Othello-GPT has an error rate of 93.29%. The main takeaway is that Othello-GPT does far better than chance in predicting legal moves when trained on both datasets. A potential explanation for these results may be that Othello-GPT is simply memorizing all possible transcripts. To test for this possibility, we created a skewed dataset of 20 million games to replace the training set of the synthetic dataset. At the beginning of every game, there are four possible opening moves: C5, D6, E3 and F4. This means the lowest layer of the game tree (first move) has four nodes (the four possible opening moves). For our skewed dataset, we truncate one of these nodes (C5), which is equivalent to removing a quarter of the whole game tree. Othello-GPT trained on the skewed dataset still yields an error rate of 0.02%. Since Othello-GPT has seen none of these test sequences before, pure sequence memorization cannot explain its performance. If the performance of Othello-GPT is not due to memorization, what is it doing? We now turn to this question by probing for internal representations of the game state. \n",
      "\n",
      "Exploring Internal Representations with Probes\n",
      "\n",
      "We seek to understand if Othello-GPT computes internal representations of the game state. One standard tool for this task is a \"probe\" (Alain et al., 2016; Belinkov et al., 2016; Tenney et al., 2019). A probe is a classifier or regressor whose input consists of internal activations from a network, and which is trained to predict a feature of interest, for example, part of speech or parse tree depth (Hewitt et al., 2019). If we are able to train an accurate probe, it suggests that a representation of the feature is encoded in the network's activations.  \n",
      "\n",
      "In our case, we want to know whether Othello-GPT's internal activations contain a representation of the current board state. To study this question, we train probes that predict the board state from the network's internal activations after a given sequence of moves. Note that the board state - whether each tile is empty or holds a black or white disc - is generally a nonlinear function of the input tokens. On the other hand, it is straightforward to write a program to compute this function, making it a natural probe target. Classifying a tile as unoccupied or occupied can be written as a linear function of the input sequence, thus we consider only the 3-way black/white/empty classifiers.\n",
      "\n",
      "We take the autoregressive features that summarize the partial sequence as the input to the probe and study results from different layers. The output is a 3-way categorical probability distribution. We randomly split pairs of internal representation and ground-truth tile state by 8:2 into training and validation sets. Error rates on the validation set are reported. A best random guess yields 52.95%, if the probe always guesses the tile is empty. \n",
      "\n",
      "Linear Probes Have High Error Rates\n",
      "\n",
      "Our first result is that linear classifier probes have poor relative accuracy. Its function can be written as p theta of x t l equals softmax of W times x t l where theta is a set containing W in the real numbers F by 3. F is the number of dimensions of input x t l. As Table 1 shows, error rates never dip below 20%. As a baseline, we have included probes trained on a randomly initialized network. Probes on the randomized network do better than chance; a constant guess of \"empty\" has a 47% error rate. But that performance is not a surprise, since it makes sense that some information about moves is preserved even by a random network. The key comparison is between the randomized network and the trained network. We can see that there is only a marginal improvement in accuracy when we move to probing a fully-trained network. This result suggests that if there is an internal representation of the board state, it does not have a simple linear form. \n",
      "\n",
      "Table 1\n",
      "Error rates (percent) of linear probes on randomized Othello-GPT and Othello-GPTs trained on different datasets across different layers (x to the power of i represents internal representations after the i-th layer). \n",
      "\n",
      "Subsection: Nonlinear Probes Have Lower Error Rates\n",
      "\n",
      "Given the poor performance of linear probes, it is natural to ask whether a nonlinear probe would have higher accuracy. Moving up one notch of complexity, we apply a 2-layer MLP as a probe. This technique has been used successfully in other language model probing work, for example, Conneau et al., 2018, Cao et al., 2021, and Hernandez et al., 2021. Its function can be written as the softmax of the matrix product of $W_1$ and the rectified linear unit (ReLU) of the matrix product of $W_2$ and $x_t^l$, where $\\theta$ is a set containing matrices $W_1$ in the real numbers with dimensions $H$ by 3, and $W_2$ in the real numbers with dimensions $F$ by $H$. $H$ is the number of hidden dimensions for the nonlinear probes.\n",
      "\n",
      "The probe accuracy for trained networks, shown in the table, is significantly better than the linear probe in absolute terms. By contrast, the baseline (probing a randomized network with nonlinear probes) shows almost no improvement over the linear case. These results indicate that the probe may be recovering a nontrivial representation of the board state in the network's activations. In the section, we describe intervention experiments validating this hypothesis.\n",
      "\n",
      "Table: Error rates (%) of nonlinear probes on randomized Othello-GPT and Othello-GPTs trained on different datasets across different layers. Standard deviations are reported in the appendix. \n",
      "\n",
      "Validating Probes with Interventional Experiments\n",
      "\n",
      "Our nonlinear probe accuracies suggest that Othello-GPT computes information reflecting the board state. It's not obvious, however, whether that information is causal for the model's predictions. To investigate this issue, we evaluate whether the representations uncovered through the probing section play a causal role in Othello-GPT's predictions. In the following section, we adhere to Belinkov et al.'s recommendation, performing a set of interventional experiments to determine the causal relationship between model predictions and the emergent world representations. To determine whether the board state information affects the network's predictions, we influence internal activations during Othello-GPT's calculation and measure the resulting effects. At a high level, the interventions are as follows: given a set of activations from the Othello-GPT, a probe predicts a baseline board state B. We record the move predictions associated with B, then modify these activations such that our probe reports an updated board state B'. Through our protocol, only a single tile s distinguishes B' from B's board state (an example of which can be seen in the intervention figure). This small modification results in a different set of possible legal moves for B'. If the new predictions match our expectations for B'---and not the predictions we recorded for B---we conclude the representation had a causal effect on the model. \n",
      "\n",
      "Explains how we intervene on a board tile. Here, we only want to flip one tile, for example, E6, from white to black. In four views presenting an Othello game in progression, which can be reliably probed from an internal representation x. The lower left board represents the model's perceived world state prior to intervention. The upper left board shows the model's predictions for legal moves given this state. Post-intervention, the model's world state is updated---E6's state has been switched from white to black (lower right), leading to a different set of legal move predictions (upper right). Note that two tiles (E6) are highlighted in the world state boards. This is the tile that we \"intervene\" on, changing from white to black. Shows our proposed intervention scheme. Light blue indicates unmodified activations; dark blue represents activations affected by intervention. Starting from a predefined layer, we intervene at the temporally-last token (shown in figure A). We replace original internal representations with the post-intervention one and resume computation for the next one layer. Part of the misinformation gets corrected (light blue), but we alternate this intervening and computation process until the last layer, from which the next-step prediction is made.   \n",
      "\n",
      "Intervention Technique\n",
      "\n",
      "To implement an intervention that changes the predicted state from a board position B to a modified version B prime we must decide (a) which layers to modify activations in, and (b) how to modify those activations. The first question is subtle. Given the causal attention mechanism of GPT-2, modifying activations for only one layer is unlikely to be effective as later layer computations incorporate information from prior board representations unaffected by our intervention. Instead, we select an initial layer L sub s then modify it and subsequent layers' activations. Critical to this experimental procedure, we must modify the internal activations at a given layer to change the predicted board state. Our modification uses a simple gradient descent method on the probe's class score for the particular tile s whose state is being modified. \n",
      "\n",
      "Consistent with the training process of probes p sub theta, we use cross entropy loss between the probe-predicted probability distribution and the desired board state, but rather than optimize probe weights theta, we optimize x for intervention. At timestep T, the internal activations of an L-layer Othello-GPT can be viewed as an L times T grid of activation vectors. Our intervention process runs Othello-GPT sequentially, but uses gradient descent to modify key activation vectors at the last timestep such that their board state class scores change. Note that if we change activations only at a middle layer, activations at higher layers are directly affected by pre-intervention information. Thus, we sequentially intervene { x sub T-1 sup l } from l equals L sub s to L at the last timestep, on all activations starting from a preset layer L sub s until the final layer. \n",
      "\n",
      "Evidence for a Causal Role for the Representation\n",
      "\n",
      "To systematically determine if this world representation is causal for model predictions, we create an evaluation benchmark set. A test case in this set consists of a triple of a partial game, a targeted board tile, and a target state. For each case, we give the partial game to Othello-GPT and perform the intervention described in the previous section. That is, we extract the model's activations mid-computation, modify them to change the representation of the targeted board tile into the target state, give back the modified world representation and let it make a prediction with this new world state.\n",
      "\n",
      "The benchmark set consists of two subsets of 1000 intervention cases: one \"natural,\" one \"unnatural.\" The natural subset consists of positions reachable by legal play. The unnatural subset contains positions that are unreachable by legal play. This second subset is designed to be a stringent test, since it is by definition far from anything encountered in the training distribution. We measure prediction alignment with the intervened world representation as a multi-label classification problem, comparing the top-N predictions against the ground-truth legal next-move set, where N is the number of legal next-moves after intervention. We then calculate an error per case (a sum of false positives and false negatives, shown in the accompanying figure). Qualitative results can be found in the appendix. Additional metrics are reported in the supplementary material. For both benchmarks, L_s=4 (intervening 5 layers) gives the best result: average errors of 0.12 and 0.06 respectively. Compared to baseline errors (2.68 and 2.59), the proposed intervention technique is effective even under unnatural board states, suggesting the emergent representations are causal to model predictions. \n",
      "\n",
      "Section: Latent Saliency Maps: Attribution via Intervention\n",
      "\n",
      "The intervention technique of the previous section provides insight into the predictions of Othello-GPT. We can also use it to create visualizations which contextualize Othello-GPT's predictions in terms of the board state. The basic idea is simple. For each tile s on the board B, we ask how much the network's prediction probability for the attributed tile p will change if we apply the intervention technique in Section 2 to change the state representation of that tile s. This will yield a value per tile s in B, positive or negative, corresponding to its saliency in the prediction of p (see Algorithm 1). We then create a visualization of the board where tiles are colored according to their saliency for the top-1 prediction for the current board state (However, ideally we can attribute any prediction). Because this map is based on the network's latent space rather than its input, we call it a \\textbf{latent saliency map}. \n",
      "\n",
      "1. Algorithm 1: Attribution via Intervention on Othello-GPT\n",
      "Input: \n",
      "- B: the current board state \n",
      "- p: a legal next move which we try to attribute \n",
      "Output: \n",
      "- {S_s}_ {s in B}: assigned sensitivity values for the prediction of p\n",
      "\n",
      "p0 gets the pre-intervention next-step probability for p. \n",
      "For each tile s in B:\n",
      "- Set tilde{x}_{t-1} as the Intervention of x_{t-1} at tile s \n",
      "- Set p_s as f_p(tilde{x}_{t-1})\n",
      "- S_s gets p0 - p_s\n",
      "\n",
      "Figure 1: Latent saliency maps\n",
      "Each subplot shows a different game state, and the top-1 prediction by the model is enclosed in a black box. Colors (red is high, blue is low) indicate the contribution of a square's state to this prediction. The contribution is higher when changing the internal representation of this square makes the prediction less likely. The values are normalized by subtracting the mean of the board.\n",
      "(A) Latent saliency maps for Othello-GPT trained on the synthetic dataset, where the model learns legal moves.\n",
      "(B) Latent saliency maps for Othello-GPT trained on the championship dataset. Rather than learning rules, this Othello-GPT learns to make strategically good moves.\n",
      "\n",
      "The visualization of latent saliency maps for the top-1 predictions for Othello-GPTs trained on the synthetic and championship datasets by intervening from Ls=4 shows a clear pattern. The synthetic Othello-GPT exhibits high saliency for precisely those tiles that are required to make a move legal. In almost all cases, other tiles have lower saliency values. Even without knowing how synthetic-GPT was trained, an experienced Othello player might be able to guess its goal. The latent saliency maps for the championship version, however, are more complex. Although tiles that relate directly to legality typically have high values, many other tiles show high saliency as well. This pattern makes sense, too. Expert moves rely on complex global features of the board. The difference between the latent saliency maps for the two versions of Othello-GPT suggests that the visualization technique is providing useful information about the two systems. \n",
      "\n",
      "Related work\n",
      "\n",
      "Our work fits into a general line of investigation into world representations created by sequence models. For example, Li et al. (2021) fine-tune two language models on synthetic natural language tasks (Long et al., 2016) and find evidence that semantic information about the underlying world state is at least weakly encoded in the activations of the network. More direct evidence of a faithful representation of 3D color space comes from Abdou et al. (2021), who examine activations in the BERT model and find a geometric connection to a standard 3D color space. Another study by Patel et al. (2022) shows that language models can learn to map conceptual domains, for example, direction and color, onto a grounded world representation via prompting techniques (Brown et al., 2020). These investigations operate in natural language domains but investigate relatively simple world models. \n",
      "\n",
      "Another related stream of work concerns neural networks that learn board games. There is a long history of work in AI to learn game moves, but in general, these systems have been given some a priori knowledge of the structure of the game. Even one of the most general-purpose game-playing engines, AlphaZero (Silver et al., 2018), has built-in knowledge of basic board structure and game rules (although, intriguingly, it seems to develop interpretable models of various strategic concepts (McGrath et al., 2021; Forde et al., 2022)). Closer to the work described here--and a major motivation for our research--is Toshniwal et al. (2021) which trains a language model on chess transcripts. They show strong evidence that transformer networks are building a representation of internal board state, but they stop short at investigating what form that representation takes.\n",
      "\n",
      "Our work can be seen as building on this line of research, with a focus on the geometry of internal representations (Section 2). The intervention technique of Section 3 follows an approach of steering model output while keeping the model frozen. It is related to the ideas behind plug-and-play controllable text generation for autoregressive (Dathathri et al., 2019; Qin et al., 2020; Krause et al., 2020) and diffusion (Li et al., 2022) language models by optimizing the likelihood of the desired attribute and the fluency of generated texts at the same time. These methods naturally involve a trade-off and require several forward and backward passes to generate. Our proposed intervention method stands out by only working on internal representations and requires only one forward pass. Finally, latent saliency maps can be viewed as a generalization of the TCAV approach (Kim et al., 2018; Ghorbani et al., 2019; Koh et al., 2020). In the TCAV setting, attribution is performed via directional derivatives. This is essentially a linearization of the gradient-descent optimization used in our attribution maps. \n",
      "\n",
      "Conclusion \n",
      "\n",
      "Our experiments provide evidence that Othello-GPT maintains a representation of game board states---that is, the Othello \"world\"---to produce sequences it was trained on. This representation appears to be nonlinear in an essential way. Further, we find that these representations can be causally linked to how the model makes its predictions. Understanding of the internal representations of a sequence model is interesting in its own right, but may also be helpful in deeper interpretations of the network. \n",
      "\n",
      "We have also described how interventional experiments may be used to create a \"latent saliency map\", which gives a picture, in terms of the Othello board, of how the network has made a prediction. Applied to two versions of Othello-GPT that were trained on different data sets, the latent saliency maps highlight the dramatic differences between underlying representations of the Othello-GPT trained on synthetic dataset and its counterpart trained on championship dataset. \n",
      "\n",
      "There are several potential lines of future work. One natural extension would be to perform the same type of investigations with other, more complex games. It would also be interesting to compare the strategies learned by a sequence model trained on game transcripts with those of a model trained with a priori knowledge of Othello. One option is to compare latent saliency maps of Othello--GPT with standard saliency maps of an Othello-playing program which has the actual board state as input. More broadly, it would be interesting to study how our results generalize to models trained on natural language. One stepping stone might be to look at language models whose training data has included game transcripts. Will we see similar representation of board state? Grammar engineering tools (Weston et al., 2015; Hermann et al., 2017; Cote et al., 2018) could help define a synthetic data generation process that maps world representations onto natural language sentences, providing a similarly controllable setting like Othello while closing the distance to natural languages. For more complex natural language tasks, can we find meaningful world representations? \n",
      "\n",
      "Our hope is that the tools described in this paper---nonlinear probes, layerwise interventions, and latent saliency maps---may prove useful in natural language settings. \n",
      "\n",
      "Acknowledgments \n",
      "\n",
      "We thank members of the Visual Computing Group and the Insight + Interaction Lab at Harvard for their early discussions and feedback. We especially thank Aoyu Wu for helping with making part of the figures and other valuable suggestions. We gratefully acknowledge the support of Harvard SEAS Fellowship (to KL), Siebel Fellowship (to AH), Open Philanthropy (to DB), and FTX Future Fund Regrant Program (to DB). This work was partially supported by NSF grant IIS-1901030. \n",
      "\n",
      "Title: Visualizing the Geometry of Probes\n",
      "\n",
      "Both linear and nonlinear probes can be viewed as geometric objects. In the case of linear probes, we can associate each classifier with the normal vector to the separating hyperplane. In the case of nonlinear probes, we can treat the second layer of the MLP as a linear classifier and take the same view. This perspective associates a vector to each grid tile, corresponding to the classifier for that grid tile.\n",
      "\n",
      "A natural question is whether these vectors display any geometric patterns. Visualizes their configuration using PCA plots. To make the connection with board geometry clear, we have overlaid a grid in which the vector for a given grid tile is connected to the vectors that represent its neighbors. At left, as a baseline, are weights of probes trained on randomized GPTs; the result is a somewhat jumbled version of the board's grid. For classifier vectors, however, we see a somewhat clearer geometric correlation with the grid shape itself. This shape may reflect a set of correlations between neighboring grid cells, and could be an interesting object for future study. One point of interest is that probe geometry for the randomized network does not seem completely random. This may fit with the fact that linear probe baseline performance is better than chance, indicating some information about board state can be gleaned from random projections of game move tokens. \n",
      "\n",
      "Section Ablation on Nonlinear Probe Accuracies. Though high accuracies have been observed on nonlinear probes in section probing, we want to develop a deeper understanding on them. For example, we wish to understand when, during a game, a model has developed world representations of board tiles, where in Othello-GPT that information is stored, how difficult it is to decode that information, and when the model may forget that information. As shown in Figure (B), we plot probe accuracies of two-layer probes varying to two different experiment settings: (1) Probe Hidden Units: the number of hidden units H in nonlinear probes; (2) Layer: at which layer the representations x to the power of l is taken out. With the increase of hidden units, i.e., probe capacity, probe accuracy is higher as it can capture more knowledge from the hidden space. For layer l, the accuracy peaks at midway, which is aligned with studies on natural language (Hewitt et al., 2019), where linguistic properties are found to be best probed in mid-layers. The format of our \"what-how-where\" plots is similar to the \"what-when-where\" plots in McGrath et al. (2021), except we ask how many hidden units within our nonlinear probe are necessary to achieve reasonable accuracy given each layer of the GPT instead of looking into the number of training epochs.\n",
      "\n",
      "We are also curious about when (in terms of game progression) these concepts are captured by Othello-GPT. Are these concepts updated immediately after each move? Will they persist or be forgotten with newer moves being made? To study this, we divide the data points for probe validation by how many steps the tile has been in its current state and plot \"what\" concept can be probed by \"how\" powerful probes \"when\" in the game progression in Figure (A). For nonlinear probes with a moderate number of hidden units, a parabolic accuracy curve is shown: concepts are best captured when they have existed for some time but not too long. This tells us: (1) forgetting happens when Othello-GPT changes its world representations; (2) there is a period of uncertainty before changes in board states are updated. \n",
      "\n",
      "Prediction heatmaps of counterfactual board states\n",
      "\n",
      "In Figure A, we can see one case of how intervention changes model prediction by intervening on the world representation of Othello-GPT. Note that the set of ground-truth legal moves are also changed by the intervention. In this case, both pre-intervention and post-intervention predictions have zero errors. Figure B shows systematic results over one thousand cases. A random subset of one hundred cases can be found in Supplementary Materials. \n",
      "\n",
      "Inter-probe Interaction \n",
      "\n",
      "In Figure 2 we show the same case as in Section 3 of the world states probed from layers of Othello-GPT starting the 5th, which is the layer we start to intervene, Ls. We can observe that after intervention is successfully done on the 5th layer at C4, the flipped disc is corrected in the immediately following layer, as seen in the pre-intervention probing results at the 6-th layer. However, when the same intervention is done on later layers again, the model is more convinced that C4 should be black and stops to correct it. A total of 100 cases can be found in Supplementary Materials, corresponding to examples in Section 3. \n",
      "\n",
      "Discussion of Latent Saliency Maps\n",
      "\n",
      "In the latent saliency maps created by attribution via intervention in Figure 1, when multiple discs are flipped, only the first one is found contributing, which is slightly misaligned to human understanding of the Othello rule. However, this is expected from the algorithm we are using because other flipped discs, even in the opposite states, still make the current prediction legal. The attribution via intervention method succeeds at visualizing the AND-logic in the Othello rule to flip one straight line: there should be opponent discs in between and same-color disc at the other end. However, when the prediction flips more than one straight line, intervening on one of them does not nullify the prediction. This is because the Othello rule is an OR-logic on top of the AND-logic: a move is legal when either one of the eight straight lines can be flipped. How to extract such knowledge with an unlimited amount of intervention experiments at hand? We leave it for future research. \n",
      "\n",
      "Title: Alternative Metrics for the Intervention Experiment\n",
      "\n",
      "Following the same multi-label classification framework that calculates the error in Figure 1, we report the F1 score in Figure 2. We measure intervention success as a multi-class classification problem, rather than a top-1 prediction problem like in the probing section, considering the fact that the latter metric is almost saturated for the Othello-GPT trained on synthetic dataset and the intervention in benchmark only cause a set difference of 2.12 tiles on average. \n",
      "\n",
      "Another line of thought to measure the alignment between model prediction and the intervened world representation is by measuring the KL divergence of the predicted next-move probability distribution from a discrete uniform prior distribution over all legal next-moves. Note that the baseline F1 and KL divergence are the averages of the F1 and KL divergence over all 1000 intervention cases, rather than treating the whole benchmark as one single multi-label classification problem or distribution distance calculation.  \n",
      "\n",
      "Error (Figure 1), F1 (Figure 2), and KL divergence (Figure 3) show a similar trend, confirming that the proposed intervention technique is useful and validating the causality from world representations to model predictions. \n",
      "\n",
      "Ablations on Intervention Hyper-parameters\n",
      "\n",
      "Experiments find this optimization process is robust to different configurations of optimizer, learning rate alpha, and number of steps. The complete world state, including states of all the 64 board tiles, is encoded in a single internal representation x, while during the intervention experiment, we only wish to change one of them. A natural question is: will the intervention operation flip tiles we are not intending? It is possible but we can mitigate that by considering the cross entropy losses of other tiles as a regularization term, weighted by a hyper-parameter beta. That is to say, the loss in main paper can be expanded as: \n",
      "\n",
      "LCE(p_theta(x), B') = Î£ s âˆˆ B â‰  B' LCE(p^s_theta(x), B'(s)) + Î²Î£ s âˆˆ B = B' LCE(p^s_theta(x), B(s)). \n",
      "\n",
      "We sweep beta at the best Ls=5 on the natural benchmark and report the average number of errors in Table 1. We can observe it does not clearly help. \n",
      "\n",
      "Table 1: Average number of errors under different betas and Ls=5 on the natural benchmark.\n",
      "\n",
      "| Beta | 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 |\n",
      "|------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|\n",
      "| Error| 0.22| 0.31| 0.77| 0.19| 0.51| 0.65| 0.42| 0.45| 0.46| 0.55|\n",
      "\n",
      "Here we further discuss another hyper-parameter ablated in Figure 1, the starting layer for intervention, Ls. If we intervene with more than 5 layers, shallow layers which have not developed reliable world representations (according to Figure 2) are touched, making intervention hazardous. On the other hand, if we intervene only at the deepest layers, though the world representation can be intervened successfully, the model does not have enough computation to adapt to the newer world representation and make predictions corresponding to it. \n",
      "\n",
      "Section: Standard Deviation of Probe Accuracy\n",
      "\n",
      "In order to exclude the possibility that the differences in probe accuracies in Table linear acc and Table nonlinear acc are caused by randomness, we run the same experiment for 100 times with different random seeds and report the standard deviations of them in Table linear var and Table nonlinear var, based on which we conclude that the probe accuracies are robust to randomness and there are significant differences between the probe accuracies of linear and nonlinear probes, and also between probes on randomized Othello-GPT and Othello-GPTs trained on different datasets.\n",
      "\n",
      "Table: Standard deviations of error rates (percent) of linear probes on randomized Othello-GPT and Othello-GPTs trained on different datasets across different layers (x^i represents internal representations after the i-th layer):\n",
      "___________________________________________\n",
      "|             | x^1 | x^2 | x^3 | x^4 | x^5 | x^6 | x^7 | x^8 |\n",
      "|-------------|-----|-----|-----|-----|-----|-----|-----|-----|\n",
      "| Randomized  | 0.04| 0.04| 0.05| 0.06| 0.06| 0.06| 0.09| 0.06|\n",
      "| Championship| 0.05| 0.04| 0.06| 0.07| 0.06| 0.06| 0.06| 0.06|\n",
      "| Synthetic   | 0.06| 0.06| 0.07| 0.06| 0.05| 0.04| 0.06| 0.08|\n",
      "-------------------------------------------\n",
      "\n",
      "Table: Standard deviations of error rates (percent) of nonlinear probes on randomized Othello-GPT and Othello-GPTs trained on different datasets across different layers (x^i represents internal representations after the i-th layer):\n",
      "_________________________________________\n",
      "|             | x^1 | x^2 | x^3 | x^4 | x^5 | x^6 | x^7 | x^8 |\n",
      "|-------------|-----|-----|-----|-----|-----|-----|-----|-----|\n",
      "| Randomized  | 0.05| 0.06| 0.07| 0.04| 0.04| 0.03| 0.04| 0.03|\n",
      "| Championship| 0.05| 0.09| 0.08| 0.06| 0.05| 0.05| 0.10| 0.06|\n",
      "| Synthetic   | 0.07| 0.09| 0.04| 0.06| 0.06| 0.09| 0.05| 0.05|\n",
      "----------------------------------------- \n",
      "\n",
      "In order to exclude the possibility that the differences in probe accuracies in Table 1 and Table 2 are caused by randomness, we run the same experiment for 100 times with different random seeds and report the standard deviations of them in Table 3 and Table 4, based on which we conclude that the probe accuracies are robust to randomness and there are significant differences between the probe accuracies of linear and nonlinear probes, and also between probes on randomized Othello-GPT and Othello-GPTs trained on different datasets.\n",
      "\n",
      "Table 3 provides the standard deviations of error rates (percent) of linear probes on randomized Othello-GPT and Othello-GPTs trained on different datasets across different layers ($x^i$ represents internal representations after the i-th layer).\n",
      "\n",
      "Table 4 displays the standard deviations of error rates (percent) of nonlinear probes on randomized Othello-GPT and Othello-GPTs trained on different datasets across different layers ($x^i$ represents internal representations after the i-th layer). \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text.split('Awk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565c6516",
   "metadata": {},
   "source": [
    "## text + audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa4c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the paths of the individual audio files\n",
    "audio_file_paths = []\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "sections = split_latex_by_section(chonk)\n",
    "for i, section in enumerate(tqdm(sections)):\n",
    "    # Generate the chat completion\n",
    "    snippet = generate_snippet(section)\n",
    "    \n",
    "    # Check if the snippet is too long\n",
    "    if len(snippet) > 4096:\n",
    "        # Split the snippet at an arbitrary newline\n",
    "        split_index = snippet.find('\\n', len(snippet) // 4)\n",
    "        snippet_part1 = snippet[:split_index]\n",
    "        snippet_part2 = snippet[split_index + 1:]\n",
    "        \n",
    "        # rerun cleaning for both parts\n",
    "        snippet_part1 = generate_snippet(snippet_part1)\n",
    "        snippet_part2 = generate_snippet(snippet_part2)\n",
    "        \n",
    "        # Generate audio for both parts\n",
    "        audio_file_paths.append(generate_audio(snippet_part1, f\"{i}_1\"))\n",
    "        audio_file_paths.append(generate_audio(snippet_part2, f\"{i}_2\"))\n",
    "        \n",
    "        text += f\"{snippet_part1} \\n\\n\"\n",
    "        text += f\"{snippet_part2} \\n\\n\"\n",
    "    else:\n",
    "        # Generate audio for the snippet\n",
    "        audio_file_paths.append(generate_audio(snippet, i))\n",
    "        text += f\"{snippet} \\n\\n\"\n",
    "\n",
    "print('concatenating audio files...')\n",
    "# Concatenate all the audio files\n",
    "combined_audio = AudioSegment.empty()\n",
    "for path in tqdm(audio_file_paths):\n",
    "    audio = AudioSegment.from_mp3(path)\n",
    "    combined_audio += audio\n",
    "\n",
    "# Export the combined audio to a single MP3 file\n",
    "combined_audio.export(\"belinkov.mp3\", format=\"mp3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
